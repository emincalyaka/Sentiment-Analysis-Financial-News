{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_project_Calyaka_Khroyan.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import string\n",
        "\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import WordNetLemmatizer, pos_tag, word_tokenize, PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "lwcKk0nqlAdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install \"git+https://github.com/javadba/mpld3@display_fix\""
      ],
      "metadata": {
        "id": "G3iHLjBJ4PP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# <font color=\"chillipepper\">**Data importation**</font>"
      ],
      "metadata": {
        "id": "vVN9O36QjqGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to form a financial strategy based on the headlines of the news concerning 7 stocks, which we have retrieved thanks to the API of Thomson Reuters.\n",
        "\n",
        "We want to do sentiment analysis on the news headlines, i.e. classify the news headlines according to whether they are positive, negative or neutral for the stock price. \n",
        "\n",
        "1) To do so, we first propose to train our NLP models on another database of financial news headlines, this one already labeled. This database comes from Kaggle.\n",
        "\n",
        "2) Second, we will use a pre-trained model for sentiment analysis in finance, finBERT, to overcome the difficulty of unlabeled data."
      ],
      "metadata": {
        "id": "p5zp0SLh0YSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In relation to stocks, we have chosen to focus on the SnP500, we have retrieved as much news as possible on these companies"
      ],
      "metadata": {
        "id": "4LW-5LrH03Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing news titles dataset\n",
        "\n",
        "! wget https://raw.githubusercontent.com/emincalyaka/NLP-financial-news/main/News_SP500.csv\n",
        "\n",
        "#Importing prelabeled dataset\n",
        "\n",
        "! wget https://github.com/emincalyaka/NLP-financial-news/raw/main/labelled%20data/FinancialPhraseBank.csv \n",
        "\n",
        "#Importing financial data \n",
        "\n",
        "! wget https://raw.githubusercontent.com/emincalyaka/NLP-financial-news/main/SPY500_Prices.csv"
      ],
      "metadata": {
        "id": "sR2R9y5s4oJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news = pd.read_csv(\"News_SP500.csv\", index_col = [\"Unnamed: 0\", \"Ticker\"])[\"text\"]\n",
        "df_news = pd.DataFrame(df_news.sort_index())\n",
        "df_news"
      ],
      "metadata": {
        "id": "dIOjjhm_AsDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_label = pd.read_csv(\"FinancialPhraseBank.csv\", names = ['label','text'],encoding='ISO-8859-1')\n",
        "data_label"
      ],
      "metadata": {
        "id": "pasVPoJYkz9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price = pd.read_csv(\"SPY500_Prices.csv\", index_col = \"date\").sort_index()\n",
        "price"
      ],
      "metadata": {
        "id": "ciqj5z6jDNo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = list(set([df_news.index[k][1] for k in range(len(df_news))]))"
      ],
      "metadata": {
        "id": "hOnbJnJEhGFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UE0etztcqo5"
      },
      "source": [
        "\n",
        "# <font color=\"chillipepper\">**Descriptive Statistics**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Content length \n"
      ],
      "metadata": {
        "id": "wuuRjgHg2rNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition of the content lengths (characters)\n",
        "%matplotlib inline\n",
        "df_news.text.drop_duplicates().apply(len).hist(bins=40)"
      ],
      "metadata": {
        "id": "Ml_64gkR2uih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One notices really short sentences compared to what one could meet in TP"
      ],
      "metadata": {
        "id": "m0gA_JvR2zho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpJnaEBbsO_T"
      },
      "source": [
        "### Lemmatization with WordnetLemmatizer, Spacy or PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "def clean_string(text, stem=\"None\"):\n",
        "\n",
        "    final_string = \"\"\n",
        "\n",
        "    # Make lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove line breaks\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    # Remove puncuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Remove stop words\n",
        "    text = text.split()\n",
        "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "    text_filtered = [word for word in text if not word in useless_words]\n",
        "\n",
        "    # Remove numbers\n",
        "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
        "\n",
        "    # Remove anything else\n",
        "    text_filtered = [ re.sub('[^A-Za-z0-9]+','', w) for w in text_filtered ]\n",
        "    text_filtered = [w for w in text_filtered if w != \"\"]\n",
        "\n",
        "    # Stem or Lemmatize\n",
        "    if stem == 'Stem':\n",
        "        stemmer = PorterStemmer() \n",
        "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
        "    elif stem == 'Lem':\n",
        "        lem = WordNetLemmatizer()\n",
        "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
        "    elif stem == 'Spacy':\n",
        "        text_filtered = nlp(' '.join(text_filtered))\n",
        "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
        "    else:\n",
        "        text_stemmed = text_filtered\n",
        "\n",
        "    final_string = ' '.join(text_stemmed)\n",
        "\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "xUza1f5kt8Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news[\"text_clean\"] = df_news.text.apply(lambda x : clean_string(x, stem = \"Spacy\"))\n",
        "df_news[\"text_clean\"]"
      ],
      "metadata": {
        "id": "b8wu6HFat-PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3A-lBI-cqpE"
      },
      "source": [
        "### Most common words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e814HCpcqpF"
      },
      "source": [
        "import wordcloud\n",
        "\n",
        "allwords = [s.split(\" \")[k] for s in df_news[\"text_clean\"] for k in range(len(s.split(\" \"))) if s.split(\" \")[k] != \"\"]\n",
        "mostcommon = FreqDist(allwords).most_common(100)\n",
        "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))\n",
        "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('Top 100 Most Common Words', fontsize=100)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostcommon_small = FreqDist(allwords).most_common(25)\n",
        "x, y = zip(*mostcommon_small)\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.margins(0.02)\n",
        "plt.bar(x, y)\n",
        "plt.xlabel('Words', fontsize=50)\n",
        "plt.ylabel('Frequency of Words', fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60, fontsize=40)\n",
        "plt.title('Frequency of 25 Most Common Words', fontsize=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ndVVGj03rhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the most frequent words do not necessarily express a feeling, in other words they are probably not very polarized. Nevertheless we have a good frequency of buy and sell words which are often associated with buy/sell signals"
      ],
      "metadata": {
        "id": "1FuV-1QPf7xU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OydwRJcOi7nK"
      },
      "source": [
        "### Bigram/Trigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xuKu0fvi7nK"
      },
      "source": [
        "c_vec = CountVectorizer(stop_words=\"english\", ngram_range=(2,3))\n",
        "# matrix of ngrams\n",
        "ngrams = c_vec.fit_transform(df_news.text_clean)\n",
        "# count frequency of ngrams\n",
        "count_values = ngrams.sum(axis = 0)\n",
        "# list of ngrams\n",
        "vocab = c_vec.vocabulary_\n",
        "df_ngram = pd.DataFrame(sorted([(count_values[0,i],k) for k,i in vocab.items()], reverse=True)\n",
        "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "df_ngram[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigrams and trigrams show nothing interesting for the most frequent of them"
      ],
      "metadata": {
        "id": "kqL_VDsL2VkI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qBb28h0h9Hj"
      },
      "source": [
        "### To try to see if the training on a pre-labeled base is relevant, we perform the previous statistics on the pre-labeled base\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_label[\"text_clean\"] = data_label[\"text\"].apply(lambda x : clean_string(x, stem = \"Spacy\"))\n",
        "data_label[\"text_clean\"]"
      ],
      "metadata": {
        "id": "p0oaIciPiAIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repartition of the content lengths (characters)\n",
        "%matplotlib inline\n",
        "data_label[\"text_clean\"].drop_duplicates().apply(len).hist(bins=40)"
      ],
      "metadata": {
        "id": "1j0FZlmL3EFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allwords = [s.split(\" \")[k] for s in data_label[\"text_clean\"] for k in range(len(s.split(\" \"))) if s.split(\" \")[k] != \"\"]\n",
        "mostcommon = FreqDist(allwords).most_common(100)\n",
        "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(mostcommon))\n",
        "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title('Top 100 Most Common Words', fontsize=100)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ol5GuEKN3MAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mostcommon_small = FreqDist(allwords).most_common(25)\n",
        "x, y = zip(*mostcommon_small)\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.margins(0.02)\n",
        "plt.bar(x, y)\n",
        "plt.xlabel('Words', fontsize=50)\n",
        "plt.ylabel('Frequency of Words', fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60, fontsize=40)\n",
        "plt.title('Frequency of 25 Most Common Words', fontsize=60)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4pdV83J3SHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_vec = CountVectorizer(stop_words=\"english\", ngram_range=(2,3))\n",
        "# matrix of ngrams\n",
        "ngrams = c_vec.fit_transform(data_label[\"text_clean\"])\n",
        "# count frequency of ngrams\n",
        "count_values = ngrams.sum(axis = 0)\n",
        "# list of ngrams\n",
        "vocab = c_vec.vocabulary_\n",
        "df_ngram = pd.DataFrame(sorted([(count_values[0,i],k) for k,i in vocab.items()], reverse=True)\n",
        "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "df_ngram[:10]"
      ],
      "metadata": {
        "id": "-SIapwaE3WKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already notice a better quality of the data:\n",
        "\n",
        "1.   The size of the sentences is more reasonable\n",
        "2.   The most frequent words have more meaning than what we have recovered\n",
        "\n",
        "This is quite problematic knowing that we want to train a model on one to classify the other\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q0IS51JK3lCp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYoGMHTPe2Y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# <font color=\"chillipepper\">**Baseline : Naive Bayes Model on prelabeled dataset**</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLYMv_yQUtGd"
      },
      "source": [
        "### We use previously lemmatized dataset ``` data_label ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQKkZ3E1UvGG"
      },
      "source": [
        "### Multinomial Naive Bayes model with Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train , X_test, y_train, y_test = train_test_split(data_label[\"text_clean\"], data_label[\"label\"], train_size = 0.7, stratify = data_label[\"label\"], shuffle = True)"
      ],
      "metadata": {
        "id": "CxvnHuRGFCoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "c_vec = CountVectorizer(binary = True, stop_words=\"english\", ngram_range=(1,1))\n",
        "\n",
        "X_train_count = c_vec.fit_transform(X_train).toarray()\n",
        "\n",
        "NB1 = MultinomialNB(alpha = 1)\n",
        "\n",
        "NB1.fit(X_train_count, y_train)"
      ],
      "metadata": {
        "id": "7CHX-MitE1hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "X_test_count = c_vec.transform(X_test).toarray()\n",
        "y_pred = NB1.predict(X_test_count)\n",
        "\n",
        "names = [\"positive\", \"neutral\", \"negative\"]\n",
        "print(classification_report(np.array(y_test).flatten(), np.array(y_pred).flatten(), target_names=names, labels=np.unique(y_pred)))"
      ],
      "metadata": {
        "id": "C3xMN2nOHjC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlcrJDASUxOd"
      },
      "source": [
        "\n",
        "### Multinomial Naive Bayes model with Tf-idf Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Yif95o-Ux6S"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(binary = True, stop_words=\"english\", ngram_range=(1,1))\n",
        "\n",
        "X_train_tfidf = tfidf_vec.fit_transform(X_train).toarray()\n",
        "\n",
        "NB2 = MultinomialNB(alpha = 1)\n",
        "\n",
        "NB2.fit(X_train_tfidf, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_tfidf = tfidf_vec.transform(X_test).toarray()\n",
        "y_pred = NB1.predict(X_test_tfidf)\n",
        "\n",
        "names = [\"positive\", \"neutral\", \"negative\"]\n",
        "print(classification_report(np.array(y_test).flatten(), np.array(y_pred).flatten(), target_names=names, labels=np.unique(y_pred)))"
      ],
      "metadata": {
        "id": "Z3ivNPu5JcL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <font color=\"chillipepper\">**Training of labeled dataset : Naive Bayes within SVM**</font>"
      ],
      "metadata": {
        "id": "1Ke0szuyJsxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf"
      ],
      "metadata": {
        "id": "odXTe3BqMBL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a ready-made module for naive bayes within svm : `nbsvm` but we won't use it here"
      ],
      "metadata": {
        "id": "M_7weDGA4Gep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(binary=True,ngram_range=(1,2)) #using unigrams, bigrams \n",
        "\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "vocab=vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "id": "BauhzwYzJu3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the log-count ratio"
      ],
      "metadata": {
        "id": "lHILtRBiEO-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vec_pos = X_train_vec[(y_train.reset_index()==\"positive\").index,:]\n",
        "X_train_vec_neg =  X_train_vec[(y_train.reset_index()==\"negative\").index,:]"
      ],
      "metadata": {
        "id": "UPQg44lYFOtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.log((X_train_vec_pos.sum(axis=0)+1)/(X_train_vec_pos.sum(0).sum()+len(vocab))/(X_train_vec_neg.sum(axis=0)+1)/(X_train_vec_neg.sum(0).sum()+len(vocab)))\n",
        "R = np.squeeze(np.asarray(R))\n",
        "R"
      ],
      "metadata": {
        "id": "-8dd-PP2B5UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "putUmnSkB7jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_nb=X_train_vec.multiply(R)\n",
        "nbsvm = LinearSVC().fit(x_nb, y_train) #Naive Bayes is an input feature\n",
        "\n",
        "y_pred = nbsvm.predict(X_test_vec.multiply(R))\n",
        "\n",
        "names = [\"positive\", \"neutral\", \"negative\"]\n",
        "print(classification_report(np.array(y_test).flatten(), np.array(y_pred).flatten(), target_names=names, labels=np.unique(y_pred)))"
      ],
      "metadata": {
        "id": "DEdTO9NWB8Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now focus on the application of these models to our unlabeled data"
      ],
      "metadata": {
        "id": "ioKHEvU7H3UN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4DFfhsvcEZ6"
      },
      "source": [
        "\n",
        "# <font color=\"chillipepper\">**Pretrained FinBERT model**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yya518/FinBERT"
      ],
      "metadata": {
        "id": "XntjfHqRIjTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "ExsDe8WsMqdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import numpy as np\n",
        "\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "sentences = [\"there is a shortage of capital, and we need extra financing\", \n",
        "             \"growth is strong and we have plenty of liquidity\", \n",
        "             \"there are doubts about our finances\", \n",
        "             \"profits are flat\"]\n",
        "\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n",
        "outputs = finbert(**inputs)[0]\n",
        "\n",
        "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
        "for idx, sent in enumerate(sentences):\n",
        "    print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])"
      ],
      "metadata": {
        "id": "VmvSqfDlMgRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSoq_yGKcYu4"
      },
      "source": [
        "\n",
        "# <font color=\"chillipepper\">**Results**</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_news"
      ],
      "metadata": {
        "id": "TM5V-spgIsim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_nb1 = NB1.predict(c_vec.transform(df_news[\"text_clean\"]).toarray())"
      ],
      "metadata": {
        "id": "p_jHtVz0Iy-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_nb2 = NB2.predict(tfidf_vec.transform(df_news[\"text_clean\"]).toarray())"
      ],
      "metadata": {
        "id": "IkTgKlVRJq6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_nbsvm = nbsvm.predict(vectorizer.transform(df_news[\"text_clean\"]).multiply(R))"
      ],
      "metadata": {
        "id": "XhhPhbEeKKua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {0:'neutral', 1:'positive',2:'negative'}\n",
        "def label_bert_ouput(x):\n",
        "  n = len(x)\n",
        "  m = 0\n",
        "  for i in range(n):\n",
        "    if x[i] > x[m]:\n",
        "      m = i\n",
        "  return labels[m]"
      ],
      "metadata": {
        "id": "Y89X0bplV8_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Very long : ~ 1 hour\n",
        "\n",
        "from tqdm import tqdm\n",
        "sentences = df_news[\"text\"].tolist()\n",
        "sentiment_finbert = []\n",
        "for k in tqdm(range(len(sentences))):\n",
        "  input = tokenizer(sentences[0], return_tensors=\"pt\", padding=True)\n",
        "  l = finbert(**input)[0].detach().numpy()[0]\n",
        "  sentiment_finbert.append(label_bert_ouput(l))"
      ],
      "metadata": {
        "id": "ffFHpdYyKphe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram of classification"
      ],
      "metadata": {
        "id": "LwLYOzDCKPCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(sentiment_nb1).value_counts()"
      ],
      "metadata": {
        "id": "VZe71XLJJiCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(sentiment_nb2).value_counts()"
      ],
      "metadata": {
        "id": "Gcp8PjDWKkKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(sentiment_nbsvm).value_counts()"
      ],
      "metadata": {
        "id": "ej5vjcm3Kmcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(sentiment_finbert).value_counts()"
      ],
      "metadata": {
        "id": "MA_wxlVxKnkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We clearly have too little oriented news to capture anything. This is particularly explicit with the finbert model which does not classify any of our news headlines as positive or negative."
      ],
      "metadata": {
        "id": "2_X1iVuw3A_k"
      }
    }
  ]
}